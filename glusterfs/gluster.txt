
# yum install -y centos-release-gluster
# yum install -y glusterfs-server
# systemctl start glusterd
# gluster peer status
# gluster peer probe gluster2
# gluster peer status
# Number of Peers: 1

# cliente
#  yum install -y centos-release-gluster
# yum install -y glusterfs-client

# Crear volumenes - directorios de bricks
# en ambos servers: mkdir /gluster (este directorio tiene que estar en una particion individual no la de /)
# g1: gluster volume list
# la cantidad de replicas maximas es la cantidad de nodos (debe ser impar >=3 split-brain)
# tantos bricks como replicas. Puede ejecutarse en cualquier server
# g1o2: gluster volume create volume1 replica 2 gluster1:/gluster/brick1 gluster2:/gluster/brick1

# la 
# gluster volume create volume1 replica 2 gluster1:/gluster/brick1 gluster2:/gluster/brick1 force

# para accederlo necesito startearlo
# gluster volume start volume1

# -------------------------------------
# acceso desde el cliente
# mount -t glusterfs gluster1:volume1 /mnt/volume1/

# Borrar el volumen
# desmontar del cliente
# gluster volume stop volume1
# gluster volume delete volume1
# manualmente borrar los archivos en c/server

# Distribuido - si no le pongo reclicas es distribuido por defecto
# gluster volume create volume2 gluster1:/gluster/brick2 gluster2:/gluster/brick2 force
# 192.168.122.121 gluster3


gluster volume create test-volume replica 3 transport tcp 
server1:/exp1 
server2:/exp2 
server3:/exp3 
server4:/exp4 
server5:/exp5 
server6:/exp6


Listing Servers
To list all nodes in the TSP:


server1# gluster pool list

Removing Servers
To remove a server from the TSP, run the following command from another server in the pool:


# gluster peer detach <server>


---------------------------------------------
Integracion kubernetes - glusterfs
---------------------------------------------
https://github.com/gluster/gluster-kubernetes

En los todos nodos k8s
/etc/modules-load.d/gluster_modules.conf
dm_snapshot
dm_mirror
dm_thin_pool


Instalacion de Heketi en nodo1 de kubernetes
--------------------------------------------

yum install heketi

sudo -u heketi ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/var/lib/heketi/.ssh/id_rsa):

[root@node1 ~]# vi /etc/heketi/heketi.json 
reemplazar
    "executor": "mock", por     "executor": "ssh",


    "_sshexec_comment": "SSH username and private key file information",
    "sshexec": {
      "keyfile": "/var/lib/heketi/.ssh/id_rsa",
      "user": "root",
      "port": "22",
      "fstab": "/etc/fstab"
    },

# enable service
systemctl enable --now heketi

# test
curl http://127.0.0.1:8080/hello
Hello from Heketi

Copio la id_rsa.pub en los authorized_key de c/nodo gluster


Instalo los client tools de heketi
yum install heketi-client
export HEKETI_CLI_SERVER=http://localhost:8080

heketi-cli cluster create gluster_cluster
Cluster id: 50aabe88586bede30fd9b442122ebe4d

heketi-cli node add --cluster=50aabe88586bede30fd9b442122ebe4d --zone=1 --management-host-name=glusterfs1 --storage-host-name=glusterfs1
Node information:
Id: 04b3033fc42000993da1006b85a421ee
State: online
Cluster Id: 50aabe88586bede30fd9b442122ebe4d
Zone: 1
Management Hostname glusterfs1
Storage Hostname glusterfs1


# add nodes
# heketi-cli node add --cluster=50aabe88586bede30fd9b442122ebe4d --zone=1 --management-host-name=glusterfs1 --storage-host-name=glusterfs1                                                            
Node information:
Id: 04b3033fc42000993da1006b85a421ee
State: online
Cluster Id: 50aabe88586bede30fd9b442122ebe4d
Zone: 1
Management Hostname glusterfs1
Storage Hostname glusterfs1
[root@node1 ~]# heketi-cli node add --cluster=50aabe88586bede30fd9b442122ebe4d --zone=1 --management-host-name=glusterfs2 --storage-host-name=glusterfs2                                                            
Node information:
Id: a746d4444971b70a9b004beec9bf0876
State: online
Cluster Id: 50aabe88586bede30fd9b442122ebe4d
Zone: 1
Management Hostname glusterfs2
Storage Hostname glusterfs2

# Add devices to nodes
----------------------


LVM  en los devices
/dev/vdb: particion en gluster  que contendra los volumes/bricks. No debe estar montado
En c/nodo gluster donde existe vdb:
wipefs -a /dev/vdb

 heketi-cli node list
Id:04b3033fc42000993da1006b85a421ee     Cluster:50aabe88586bede30fd9b442122ebe4d
Id:a746d4444971b70a9b004beec9bf0876     Cluster:50aabe88586bede30fd9b442122ebe4d

heketi-cli device add --name=/dev/vdb --node=04b3033fc42000993da1006b85a421ee
Device added successfully
heketi-cli device add --name=/dev/vdb --node=a746d4444971b70a9b004beec9bf0876
Device added successfully

Ahora podemos hacer el call para allocar espacio
------------------------------------------------
# minimo son 4gigas
heketi-cli volume create --size=4 --replica=2

Name: vol_291e480acbd42020fdef6cc37c424918
Size: 4
Volume Id: 291e480acbd42020fdef6cc37c424918
Cluster Id: 50aabe88586bede30fd9b442122ebe4d
Mount: glusterfs1:vol_291e480acbd42020fdef6cc37c424918
Mount Options: backup-volfile-servers=glusterfs2
Block: false
Free Size: 0
Reserved Size: 0
Block Hosting Restriction: (none)
Block Volumes: []
Durability Type: replicate
Distribute Count: 1
Replica Count: 2

# Test
mount -t glusterfs -o backup-volfile-servers=glusterfs2 glusterfs1:vol_291e480acbd42020fdef6cc37c424918 /mnt

# En el nodo gluster si voy a /var/lib/heketi/mounts puedo ver las carpetas que representan los ids y los archivos

